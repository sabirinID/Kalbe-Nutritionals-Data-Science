# -*- coding: utf-8 -*-
"""Data_Modeling_KMeans_Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vIGbrW7Nlxpnr7Ofn2P0EGBo-JSfzNGp

# Unsupervised Learning: K-Means Clustering
---

# **K-Means Clustering**

Referensi:
- https://www.ibm.com/topics/unsupervised-learning
- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html

## Import Library
"""

# Data manipulation
import numpy as np
import pandas as pd
import datetime as dt

# Data visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Machine Learning
import sklearn as sk

# Ignore warning
import warnings
warnings.filterwarnings('ignore')

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

print('NumPy', np.__version__)
print('Pandas', pd.__version__)
print('Matplotlib', mpl.__version__)
print('Seaborn', sns.__version__)
print('Scikit-Learn', sk.__version__)

"""## Read Dataset"""

from google.colab import drive
drive.mount('/content/drive')

# Read the data files
data = pd.read_csv('/content/drive/MyDrive/Kalbe Farma/Cleaned_Data.csv')

data.sample(5)

data.shape

data.info()

data.columns

# Group by 'CustomerID' and .agg() the columns
data_clustering = data.groupby('CustomerID').agg({
    'TransactionID': 'count',
    'Quantity': 'sum',
    'TotalAmount': 'sum'
}).reset_index()

# Rename the columns for clarity
data_clustering.columns = ['CustomerID', 'TotalTransaction', 'TotalQuantity', 'TotalAmount']

data_clustering.head()

data_clustering.shape

dc = data_clustering.copy()

from matplotlib import rcParams

rcParams['figure.figsize'] = 16, 9

for i in range(0, len(dc.columns)):
    plt.subplot(2, 2, i + 1)
    sns.kdeplot(x=dc[dc.columns[i]],
                color='cyan')

    plt.title('Distribution of ' + dc.columns[i],
              fontsize=12,
              fontweight='bold')
    plt.xlabel(dc.columns[i])
    plt.tight_layout(pad=2)

for i in range(0, len(dc.columns)):
    plt.subplot(2, 2, i + 1)
    sns.boxplot(y=dc[dc.columns[i]],
                color='violet')

    plt.title('Distribution of ' + dc.columns[i],
              fontsize=12,
              fontweight='bold')
    plt.tight_layout(pad=2)

for i in range(0, len(dc.columns)):
    plt.subplot(2, 2, i + 1)
    sns.violinplot(y=dc[dc.columns[i]],
                   color='magenta')

    plt.title('Distribution of ' + dc.columns[i],
              fontsize=12,
              fontweight='bold')
    plt.tight_layout(pad=2)

plt.figure(figsize=(16, 16))

sns.set_style('white')
sns.set_palette('viridis')
sns.pairplot(data=dc,
             diag_kind='kde',
             corner=True)

plt.show()

plt.figure(figsize=(16, 16))

sns.heatmap(data=dc.corr(),
            cmap='viridis',
            annot=True,
            fmt='.2f',
            linewidths=0.5)
plt.title(label='Heatmap - Correlation Matrix',
          fontsize=16,
          fontweight='bold')
plt.show()

"""## Feature Scaling: Standardization"""

model = dc.drop(columns='CustomerID').copy()
model.columns

features = ['txn', 'qty', 'amt']

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaled_data = scaler.fit_transform(model) # Fitting and transforming the data
scaled_df = pd.DataFrame(scaled_data, columns=features)
scaled_df.head()

scaled_df.describe()

"""âœ… Nilai mean mendekati 0 dan std mendekati 1.

## Modeling and Evaluation
"""

df_model = scaled_df.copy()

plt.figure(figsize=(16, 16))

sns.set_style('white')
sns.set_palette('viridis')
sns.pairplot(data=df_model,
             diag_kind='kde',
             corner=True)

plt.show()

"""### Inertia Score"""

from sklearn.cluster import KMeans

inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10,
                    max_iter=200, random_state=100)
    kmeans.fit(df_model)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(16, 9))
plt.plot(range(1, 11),
         inertia,
         color='tab:blue',
         linewidth=2.5,
         marker='o',
         markerfacecolor='tab:red',
         markersize=10)
plt.xlabel('k', fontsize=14)
plt.ylabel('Inertia', fontsize=14)
plt.style.use('ggplot')
plt.title('Inertia Score Elbow for KMeans Clustering', fontsize=16)
plt.show()

(pd.Series(inertia) - pd.Series(inertia).shift(-1)) / pd.Series(inertia) * 100

"""ðŸ”Ž Observasi
- Dengan parameter inertia score, nilai optimal pada k = 3, karena terjadi penurunan inertia yang tidak terlalu signifikan setelah itu.

### Distortion Score
"""

# pip install yellowbrick

from yellowbrick.cluster import KElbowVisualizer

# Fit model
model = KMeans(init='k-means++', n_init=10, max_iter=200, random_state=100)
visualizer = KElbowVisualizer(model, k=(1, 11), metric='distortion', timings=True, locate_elbow=True)
visualizer.fit(df_model)
visualizer.show()

"""ðŸ”Ž Observasi
- Dengan parameter distortion score, nilai optimal pada k = 3 dengan skor = 316,968.

### Silhouette Score
"""

from yellowbrick.cluster import KElbowVisualizer

model = KMeans(init='k-means++', n_init=10, max_iter=200, random_state=100)
visualizer = KElbowVisualizer(model, k=(2, 11), metric='silhouette', timings=True, locate_elbow=True)
visualizer.fit(df_model)
visualizer.show()

"""ðŸ”Ž Observasi
- Dengan parameter silhouette score, nilai optimal pada k = 2 dengan skor = 0,485.

### Silhouette Plot
"""

from yellowbrick.cluster import SilhouetteVisualizer

for i in [2, 3, 4, 5]:
    model = KMeans(i, init='k-means++', n_init=10, max_iter=200, random_state=100)
    visualizer = SilhouetteVisualizer(model, colors='viridis')
    visualizer.fit(df_model)
    visualizer.show()

from sklearn.metrics import silhouette_score

def visualize_silhouette_layer(data):
    clusters_range = range(2, 10)
    results = []

    for i in clusters_range:
        km = KMeans(n_clusters=i, random_state=100)
        cluster_labels = km.fit_predict(data)
        silhouette_avg = silhouette_score(data, cluster_labels)
        results.append([i, silhouette_avg])

    result = pd.DataFrame(results, columns=['n_clusters', 'silhouette_score'])
    pivot_km = pd.pivot_table(result, index='n_clusters', values='silhouette_score')

    plt.figure(figsize=(9, 16))
    sns.heatmap(pivot_km, annot=True, linewidths=1, fmt='.3f', cmap='viridis')
    plt.tight_layout()
    plt.title('Silhouette Score of K-means Clustering', fontsize=16, fontweight='bold')
    plt.show()

visualize_silhouette_layer(df_model)

"""ðŸ”Ž Observasi
- Seperti yang dapat dilihat, berdasarkan analisis yang dilakukan, menunjukkan hasil yang cukup mirip, yaitu nilai optimal pada k = 2. Namun, kita akan menggunakan k = 3 sebagai jumlah klaster pada klasterisasi K-Means, karena pertimbangan berikut:
  - Distortion score
  - Silhouette score (0,429)
- Selain itu, k = 3 lebih baik digunakan untuk segmentasi pelanggan (tidak terlalu sedikit dan tidak terlalu banyak).
"""

model = df_model.copy()

from sklearn.cluster import KMeans

km = KMeans(n_clusters=3,
            init='k-means++',
            n_init=10,
            max_iter=200,
            random_state=100)
km.fit(model)

"""### Summary Statistics"""

# Labels
model['Segment'] = km.labels_
model.groupby('Segment').agg(['std', 'mean', 'median'])

model.groupby('Segment').count()

# Centroids
km.cluster_centers_

"""## Customer Segmentation Analysis"""

plt.figure(figsize=(16, 9))

fig = px.scatter_3d(data_frame=model,
                    x='txn',
                    y='qty',
                    z='amt',
                    color='Segment',
                    color_continuous_scale='viridis',
                    size_max=10)

fig.update_layout(title='Customer Segmentation', title_font_size=16)

fig.show()

"""## Principal Component Analysis (PCA)"""

from sklearn.decomposition import PCA

pca = PCA(n_components=4)
pca.fit(model)
pca_tf = pca.transform(model)

pca_df = pd.DataFrame(pca_tf, columns=['pc1', 'pc2', 'pc3', 'pc4'])
pca_df.describe()

print('Explained variance:', pca.explained_variance_)
print('Explained variance ratio:', pca.explained_variance_ratio_)

plt.figure(figsize=(16, 9))
plt.bar(['pc1', 'pc2', 'pc3', 'pc4'], pca.explained_variance_)
plt.bar(['pc1', 'pc2', 'pc3', 'pc4'], pca.explained_variance_ratio_)

plt.figure(figsize=(16, 16))

sns.set_style('white')
sns.pairplot(data=pca_df,
             palette='viridis',
             diag_kind='kde',
             corner=True)

plt.show()

from sklearn.decomposition import PCA
pca = PCA(n_components=2)

pca.fit(model)
pca_tf = pca.transform(model)
pca_df = pd.DataFrame(data=pca_tf, columns=['pc1', 'pc2'])
pca_df['Segment'] = model['Segment']
pca_df.head()

plt.figure(figsize=(16, 16))

sns.set_style('white')
sns.pairplot(data=model,
             hue='Segment',
             palette='viridis',
             diag_kind='kde',
             corner=True)

plt.show()

plt.figure(figsize=(16, 16))

sns.set_style('white')
sns.pairplot(data=pca_df,
             hue='Segment',
             palette='viridis',
             diag_kind='kde',
             corner=True)

plt.show()

plt.figure(figsize=(16, 9))

sns.scatterplot(data=pca_df,
                x='pc1',
                y='pc2',
                hue='Segment',
                palette='viridis')
plt.title('Customer Segmentation', fontsize=20)
plt.tight_layout()

plt.show()

"""## Data Summary"""

data = data_clustering.copy()
data['Segment'] = km.labels_

features = ['TotalTransaction', 'TotalQuantity', 'TotalAmount', 'Segment']
data_summary = data[features]

round(data_summary.groupby('Segment').agg(['median']), 2).round()

colors = plt.cm.viridis([0.1, 0.5, 0.9])
# colors = plt.cm.viridis([0.1, 0.4, 0.7, 1.0])

def dist_list(list):
    plt.figure(figsize=[16, 9])
    i = 1
    for col in list:
        ax = plt.subplot(1, 3, i)
        ax.vlines(data[col].median(), ymin=-0.5, ymax=2.5, color='black', linestyle='--')
        g = data.groupby('Segment')
        x = g[col].median().index
        y = g[col].median().values
        ax.barh(x, y, color=colors)
        plt.title(col)
        plt.ylabel('Segment')
        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))
        i = i + 1

dist_list(['TotalTransaction', 'TotalQuantity', 'TotalAmount'])
plt.tight_layout(pad=2)
plt.show()

"""ðŸ”Ž Insights
- Berdasarkan hasil K-Mean Clustering, dapat disimpulkan bahwa karakteristik dari setiap klaster adalah sebagai berikut.
  - Klaster 0
    - Total transaksi sebanyak 11 kali (sebanding dengan rerata transaksi per tahun).
    - Total kuantitas produk sebanyak 41 buah.
    - Total pengeluaran sebesar Rp362.300/tahun.
  - Klaster 1
    - Total transaksi sebanyak 15 kali (di atas rerata transaksi per tahun).
    - Total kuantitas produk sebanyak 57 buah.
    - Total pengeluaran sebesar Rp509.900/tahun.
  - Klaster 2
    - Total transaksi sebanyak 8 kali (di bawah rerata transaksi per tahun).
    - Total kuantitas produk sebanyak 28 buah.
    - Total pengeluaran sebesar Rp234.550/tahun.
- Dari karakteriktik tersebut, masing-masing klaster dapat dimasukkan ke dalam segmen berikut.
  - Klaster 0: Segmen *Moderate Customer*
  - Klaster 1: Segmen *High Customer*
  - Klaster 2: Segmen *Low Customer*
"""

segment_med = data.groupby('Segment').median().reset_index()

df_melt = pd.melt(segment_med.reset_index(),
                  id_vars='Segment',
                  value_vars=['TotalTransaction', 'TotalQuantity', 'TotalAmount'],
                  var_name='Metric',
                  value_name='Value')

plt.figure(figsize=(16, 9))
sns.pointplot(data=df_melt, x='Metric', y='Value', hue='Segment', palette='viridis')
plt.title('Pattern of Customer', fontsize=20)
plt.xlabel('Metric', fontsize=14)
plt.ylabel('Value', fontsize=14)
plt.show()

"""## Model Interpretation and Recommendation

### Percentage of Segment Population
"""

segment_percent = data_summary['Segment'].value_counts().reset_index()
segment_percent.columns = ['Segment', 'Count']
segment_percent['Percentage (%)'] = round((segment_percent['Count'] / len(data_summary)) * 100, 1)
segment_percent = segment_percent.sort_values(by=['Segment']).reset_index(drop=True)
segment_percent

fig, ax = plt.subplots(figsize=(16, 9))
bars = plt.bar(x=segment_percent['Segment'], height=segment_percent['Percentage (%)'], color=colors)

plt.title('Percentage of Segment Population', fontsize=20, fontweight='bold')
plt.bar_label(ax.containers[0], fmt='%.0f%%', fontsize=18)
plt.xlabel('Segment', fontsize=16)
plt.xticks(range(0, 3))
plt.ylabel('Percentage (%)', fontsize=16)
plt.ylim(0, 50)
plt.show()

"""ðŸ”Ž Insights
- Berdasarkan hasil analisis, diketahui bahwa:
  - Segmen *Moderate Customer*
    - Segmen ini merupakan segmen terbesar dari keseluruhan pelanggan, mencakup 45% dari total pelanggan.
    - Karakteristik utama dari segmen ini adalah memiliki jumlah pembelian dan pengeluaran yang sedang.
    - Perusahaan dapat memanfaatkan potensi pertumbuhan dari segmen ini dengan memberikan perhatian lebih pada mereka.
  - Segmen *High Customer*
    - Segmen ini memiliki jumlah pembelian dan pengeluaran yang tinggi, meskipun memiliki populasi yang paling kecil dari keseluruhan pelanggan.
    - Segmen ini memiliki potensi besar untuk memberikan kontribusi pendapatan yang signifikan bagi perusahaan.
    - Perusahaan dapat merancang strategi bisnis yang efektif untuk menjangkau segmen ini untuk memaksimalkan potensi pertumbuhan dari mereka.
  - Segmen *Low Customer*
    - Segmen ini memiliki jumlah pembelian dan pengeluaran yang rendah, dan memiliki transaksi paling rendah dari keseluruhan pelanggan.
    - Namun, potensi peningkatan transaksi masih ada, sehingga perusahaan dapat memperkuat aktivitas pembelian dari segmen ini.
    - Perusahaan dapat mendorong peningkatan transaksi dengan memberikan penawaran menarik kepada segmen ini.
"""